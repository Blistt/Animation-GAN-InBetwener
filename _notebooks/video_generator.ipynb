{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This notebook reads an mp4 video file and extracts all the individual frames.\n",
    "'''\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from _scripts.generate_from_vid import generate\n",
    "\n",
    "def interpolate_video(path, model, thresh=[0.65, 0.97], normalize=True, double_fps=False):\n",
    "    new_frames = []\n",
    "    # Read in the video\n",
    "    vidcap = cv2.VideoCapture(path)\n",
    "    print('Video has', int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT)), 'frames')\n",
    "    success, prev_frame = vidcap.read()\n",
    "    prev_frame = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    if normalize:\n",
    "        prev_frame = prev_frame / np.max(prev_frame)\n",
    "        prev_frame[prev_frame < 0.75] = 0\n",
    "        prev_frame[prev_frame >= 0.75] = 1\n",
    "    unique_frame = prev_frame.copy()\n",
    "    target_frame = prev_frame.copy()\n",
    "    i = 0\n",
    "    num_unique_frames = 0\n",
    "    num_duplicate_frames = 0\n",
    "    num_shot_boundaries = 0\n",
    "    success = True\n",
    "    generated_frame = None\n",
    "    while success:\n",
    "        success,curr_frame = vidcap.read()\n",
    "        # Display the read image, not save, just display\n",
    "        if success:\n",
    "            curr_frame = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n",
    "            curr_frame = curr_frame / np.max(curr_frame)\n",
    "            curr_frame[curr_frame < 0.75] = 0\n",
    "            curr_frame[curr_frame >= 0.75] = 1\n",
    "            ssim_diff = ssim(prev_frame, curr_frame, data_range=curr_frame.max() - curr_frame.min(),\n",
    "                             channel_axis=None)\n",
    "            unique = thresh[0] <= ssim_diff <= thresh[1]\n",
    "            duplicate = ssim_diff > thresh[1]\n",
    "            shot_boundary = ssim_diff < thresh[0]\n",
    "            if unique:\n",
    "              unique_frame = prev_frame.copy()\n",
    "              num_unique_frames += 1\n",
    "              target_frame = unique_frame.copy()\n",
    "            if duplicate:\n",
    "              num_duplicate_frames += 1\n",
    "              if not double_fps:\n",
    "                target_frame = generate(model, new_frames[-1], curr_frame)\n",
    "            if shot_boundary:\n",
    "              target_frame = prev_frame.copy()\n",
    "              num_shot_boundaries += 1\n",
    "            if i == 1:\n",
    "              target_frame = prev_frame \n",
    "            if double_fps and not shot_boundary:\n",
    "               generated_frame = generate(model, prev_frame, curr_frame)   \n",
    "        new_frames.append(target_frame)\n",
    "        if generated_frame is not None:\n",
    "            new_frames.append(generated_frame)\n",
    "        prev_frame = curr_frame\n",
    "        print(i)\n",
    "        i += 1\n",
    "    print('Video has', num_unique_frames, 'unique frames')\n",
    "    print('Video has', num_duplicate_frames, 'duplicate frames')\n",
    "    print('Video has', num_shot_boundaries, 'shot boundaries')\n",
    "    print('Interpolated video has', len(new_frames), 'frames')\n",
    "    return new_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using UNetPadded\n",
      "Video has 97 frames\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "Video has 90 unique frames\n",
      "Video has 6 duplicate frames\n",
      "Video has 0 shot boundaries\n",
      "Interpolated video has 194 frames\n",
      "Extracted 194 frames\n"
     ]
    }
   ],
   "source": [
    "from _generators.generator_padded import UNetPadded\n",
    "import torch\n",
    "\n",
    "thresh = [0.65, 0.95]\n",
    "model = UNetPadded(2, output_channels=1, hidden_channels=64)\n",
    "checkpoint_path = '/data/farriaga/Experiments/exp_paddedd/gen_checkpoint.pth'\n",
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "model.eval()\n",
    "new_frames = interpolate_video('test - Trim.mp4', model=model, thresh=thresh,\n",
    "                               double_fps=True, normalize=True)\n",
    "print('Extracted', len(new_frames), 'frames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/data/farriaga/Experiments/video_results'\n",
    "for i, frame in enumerate(new_frames):\n",
    "    cv2.imwrite(f'{path}/frame_{i}.png', frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "fps = 48\n",
    "output_video_path = '/data/farriaga/Experiments/video/test_double_fps.mp4'\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (512, 288))\n",
    "for frame in new_frames:\n",
    "    frame = (frame * 255).astype(np.uint8)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)\n",
    "    out.write(frame)\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "import imageio\n",
    "\n",
    "def video_to_gif(video_path, gif_path):\n",
    "    # Read the video file\n",
    "    video = imageio.get_reader(video_path)\n",
    "    fps = video.get_meta_data()['fps']  # Get the frames per second\n",
    "    duration = 1.0 / fps  # Calculate the duration of each frame\n",
    "\n",
    "    # Write the output GIF file\n",
    "    with imageio.get_writer(gif_path, mode='I', duration=duration) as writer:\n",
    "        for frame in video:\n",
    "            writer.append_data(frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_to_gif('/data/farriaga/Experiments/video/test_double_fps.mp4', \n",
    "             '/data/farriaga/Experiments/video/test_double_fps.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/farriaga/gan-interpolator/_notebooks/video_generator.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bbilbo.utep.edu/home/farriaga/gan-interpolator/_notebooks/video_generator.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m_line_extractor\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mextract\u001b[39;00m \u001b[39mimport\u001b[39;00m extract_from_video\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbilbo.utep.edu/home/farriaga/gan-interpolator/_notebooks/video_generator.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m_generators\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgenerator_padded\u001b[39;00m \u001b[39mimport\u001b[39;00m UNetPadded\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bbilbo.utep.edu/home/farriaga/gan-interpolator/_notebooks/video_generator.ipynb#W6sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n",
      "File \u001b[0;32m~/gan-interpolator/_notebooks/../_line_extractor/extract.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcv2\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodel\u001b[39;00m \u001b[39mimport\u001b[39;00m SketchKeras\n\u001b[1;32m     10\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpathlib\u001b[39;00m \u001b[39mimport\u001b[39;00m Path\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'model'"
     ]
    }
   ],
   "source": [
    "from _line_extractor.extract import extract_from_video\n",
    "from _generators.generator_padded import UNetPadded\n",
    "import torch\n",
    "\n",
    "model = UNetPadded(2, output_channels=1, hidden_channels=64)\n",
    "checkpoint_path = '/data/farriaga/Experiments/exp_paddedd/gen_checkpoint.pth'\n",
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "model.eval()\n",
    "\n",
    "extract_from_video('jigokuraku.mp4', model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
