{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This notebook reads an mp4 video file and extracts all the individual frames.\n",
    "'''\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from scripts.generate_from_vid import generate\n",
    "\n",
    "def interpolate_video(path, model, thresh=[0.65, 0.97], normalize=True, double_fps=False):\n",
    "    new_frames = []\n",
    "    # Read in the video\n",
    "    vidcap = cv2.VideoCapture(path)\n",
    "    print('Video has', int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT)), 'frames')\n",
    "    success, prev_frame = vidcap.read()\n",
    "    prev_frame = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "    if normalize:\n",
    "        prev_frame = prev_frame / np.max(prev_frame)\n",
    "        prev_frame[prev_frame < 0.75] = 0\n",
    "        prev_frame[prev_frame >= 0.75] = 1\n",
    "    unique_frame = prev_frame.copy()\n",
    "    target_frame = prev_frame.copy()\n",
    "    i = 0\n",
    "    num_unique_frames = 0\n",
    "    num_duplicate_frames = 0\n",
    "    num_shot_boundaries = 0\n",
    "    success = True\n",
    "    generated_frame = None\n",
    "    while success:\n",
    "        success,curr_frame = vidcap.read()\n",
    "        # Display the read image, not save, just display\n",
    "        if success:\n",
    "            curr_frame = cv2.cvtColor(curr_frame, cv2.COLOR_BGR2GRAY)\n",
    "            curr_frame = curr_frame / np.max(curr_frame)\n",
    "            curr_frame[curr_frame < 0.75] = 0\n",
    "            curr_frame[curr_frame >= 0.75] = 1\n",
    "            ssim_diff = ssim(prev_frame, curr_frame, data_range=curr_frame.max() - curr_frame.min(),\n",
    "                             channel_axis=None)\n",
    "            unique = thresh[0] <= ssim_diff <= thresh[1]\n",
    "            duplicate = ssim_diff > thresh[1]\n",
    "            shot_boundary = ssim_diff < thresh[0]\n",
    "            if unique:\n",
    "              unique_frame = prev_frame.copy()\n",
    "              num_unique_frames += 1\n",
    "              target_frame = unique_frame.copy()\n",
    "            if duplicate:\n",
    "              num_duplicate_frames += 1\n",
    "              if not double_fps:\n",
    "                target_frame = generate(model, new_frames[-1], curr_frame)\n",
    "            if shot_boundary:\n",
    "              target_frame = prev_frame.copy()\n",
    "              num_shot_boundaries += 1\n",
    "            if i == 1:\n",
    "              target_frame = prev_frame \n",
    "            if double_fps and not shot_boundary:\n",
    "               generated_frame = generate(model, prev_frame, curr_frame)   \n",
    "        new_frames.append(target_frame)\n",
    "        if generated_frame is not None:\n",
    "            new_frames.append(generated_frame)\n",
    "        prev_frame = curr_frame\n",
    "        print(i)\n",
    "        i += 1\n",
    "    print('Video has', num_unique_frames, 'unique frames')\n",
    "    print('Video has', num_duplicate_frames, 'duplicate frames')\n",
    "    print('Video has', num_shot_boundaries, 'shot boundaries')\n",
    "    print('Interpolated video has', len(new_frames), 'frames')\n",
    "    return new_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using UNetPadded\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/data/farriaga/Experiments/exp_paddedd/gen_checkpoint.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m UNetPadded(\u001b[38;5;241m2\u001b[39m, output_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, hidden_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n\u001b[1;32m      6\u001b[0m checkpoint_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/farriaga/Experiments/exp_paddedd/gen_checkpoint.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 7\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(checkpoint_path))\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      9\u001b[0m new_frames \u001b[38;5;241m=\u001b[39m interpolate_video(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest - Trim.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, thresh\u001b[38;5;241m=\u001b[39mthresh,\n\u001b[1;32m     10\u001b[0m                                double_fps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.11/site-packages/torch/serialization.py:791\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    789\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 791\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_file_like(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    793\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    795\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    796\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.11/site-packages/torch/serialization.py:271\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 271\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _open_file(name_or_buffer, mode)\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    273\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch_env/lib/python3.11/site-packages/torch/serialization.py:252\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mopen\u001b[39m(name, mode))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/data/farriaga/Experiments/exp_paddedd/gen_checkpoint.pth'"
     ]
    }
   ],
   "source": [
    "from generators.generator_padded import UNetPadded\n",
    "import torch\n",
    "\n",
    "thresh = [0.65, 0.95]\n",
    "model = UNetPadded(2, output_channels=1, hidden_channels=64)\n",
    "checkpoint_path = '/data/farriaga/Experiments/exp_paddedd/gen_checkpoint.pth'\n",
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "model.eval()\n",
    "new_frames = interpolate_video('test - Trim.mp4', model=model, thresh=thresh,\n",
    "                               double_fps=True, normalize=True)\n",
    "print('Extracted', len(new_frames), 'frames')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/data/farriaga/Experiments/video_results'\n",
    "for i, frame in enumerate(new_frames):\n",
    "    cv2.imwrite(f'{path}/frame_{i}.png', frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "fps = 48\n",
    "output_video_path = '/data/farriaga/Experiments/video/test_double_fps.mp4'\n",
    "out = cv2.VideoWriter(output_video_path, fourcc, fps, (512, 288))\n",
    "for frame in new_frames:\n",
    "    frame = (frame * 255).astype(np.uint8)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2BGR)\n",
    "    out.write(frame)\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "import imageio\n",
    "\n",
    "def video_to_gif(video_path, gif_path):\n",
    "    # Read the video file\n",
    "    video = imageio.get_reader(video_path)\n",
    "    fps = video.get_meta_data()['fps']  # Get the frames per second\n",
    "    duration = 1.0 / fps  # Calculate the duration of each frame\n",
    "\n",
    "    # Write the output GIF file\n",
    "    with imageio.get_writer(gif_path, mode='I', duration=duration) as writer:\n",
    "        for frame in video:\n",
    "            writer.append_data(frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_to_gif('/data/farriaga/Experiments/video/test_double_fps.mp4', \n",
    "             '/data/farriaga/Experiments/video/test_double_fps.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mline_extractor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextract\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m extract_from_video\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgenerators\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerator_padded\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UNetPadded\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[0;32m~/gan-interpolator/notebooks/../line_extractor/extract.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SketchKeras\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'model'"
     ]
    }
   ],
   "source": [
    "from line_extractor.extract import extract_from_video\n",
    "from generators.generator_padded import UNetPadded\n",
    "import torch\n",
    "\n",
    "model = UNetPadded(2, output_channels=1, hidden_channels=64)\n",
    "checkpoint_path = '/data/farriaga/Experiments/exp_paddedd/gen_checkpoint.pth'\n",
    "model.load_state_dict(torch.load(checkpoint_path))\n",
    "model.eval()\n",
    "\n",
    "# extract_from_video('jigokuraku.mp4', model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
